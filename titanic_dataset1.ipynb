{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b4fd0c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression weights: [-0.25792615  0.06659689 -0.15585172  0.01007019  0.04598599]\n",
      "Logistic Regression bias: 0.09728698783384906\n",
      "SVM Model coefficients: [[-1.97159209 -0.03796871 -1.14434574 -0.01956976  0.08214055]]\n",
      "SVM Model intercept: [1.56587129]\n",
      "Decision Tree Model: ((0, 0.8841361528584817), ((2, 0.729833063403976), ((2, 0.06620757411746114), ((0, 0.5250987204073212), 0, 1), ((2, 0.12135904814461185), 1, ((0, 0.1382533835890013), 1, 1))), ((2, 0.8338658704229875), 0, ((3, 0.3855816015991014), 1, ((1, 0.689997518623512), 0, 1)))), 0)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Load Titanic dataset (dummy data for demonstration)\n",
    "# Assume the dataset has features X and labels y\n",
    "# Here, X contains features like age, gender, etc. and y contains survival status (0 or 1)\n",
    "# You need to replace X and y with your actual dataset\n",
    "X = np.random.rand(100, 5)  # Example: 100 samples, 5 features\n",
    "y = np.random.randint(2, size=100)  # Example: Binary labels (survived or not)\n",
    "\n",
    "# Logistic Regression\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def logistic_regression_train(X, y, learning_rate=0.01, epochs=1000):\n",
    "    # Initialize weights and bias\n",
    "    n_samples, n_features = X.shape\n",
    "    weights = np.zeros(n_features)\n",
    "    bias = 0\n",
    "\n",
    "    # Gradient descent\n",
    "    for _ in range(epochs):\n",
    "        linear_model = np.dot(X, weights) + bias\n",
    "        y_predicted = sigmoid(linear_model)\n",
    "\n",
    "        # Gradient calculation\n",
    "        dw = (1 / n_samples) * np.dot(X.T, (y_predicted - y))\n",
    "        db = (1 / n_samples) * np.sum(y_predicted - y)\n",
    "\n",
    "        # Update weights and bias\n",
    "        weights -= learning_rate * dw\n",
    "        bias -= learning_rate * db\n",
    "\n",
    "    return weights, bias\n",
    "\n",
    "# Train logistic regression model\n",
    "weights, bias = logistic_regression_train(X, y)\n",
    "print(\"Logistic Regression weights:\", weights)\n",
    "print(\"Logistic Regression bias:\", bias)\n",
    "\n",
    "# SVM (Support Vector Machine)\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Train SVM model\n",
    "svm_model = SVC(kernel='linear')\n",
    "svm_model.fit(X, y)\n",
    "print(\"SVM Model coefficients:\", svm_model.coef_)\n",
    "print(\"SVM Model intercept:\", svm_model.intercept_)\n",
    "\n",
    "# Decision Tree\n",
    "def entropy(y):\n",
    "    n_samples = len(y)\n",
    "    _, counts = np.unique(y, return_counts=True)\n",
    "    probabilities = counts / n_samples\n",
    "    entropy_value = -np.sum(probabilities * np.log2(probabilities))\n",
    "    return entropy_value\n",
    "\n",
    "def information_gain(X, y, feature_index, threshold):\n",
    "    left_indices = np.where(X[:, feature_index] <= threshold)\n",
    "    right_indices = np.where(X[:, feature_index] > threshold)\n",
    "\n",
    "    left_y = y[left_indices]\n",
    "    right_y = y[right_indices]\n",
    "\n",
    "    p_left = len(left_y) / len(y)\n",
    "    p_right = len(right_y) / len(y)\n",
    "\n",
    "    gain = entropy(y) - (p_left * entropy(left_y) + p_right * entropy(right_y))\n",
    "    return gain\n",
    "\n",
    "def find_best_split(X, y):\n",
    "    n_samples, n_features = X.shape\n",
    "    best_gain = 0\n",
    "    best_feature_index = None\n",
    "    best_threshold = None\n",
    "\n",
    "    for feature_index in range(n_features):\n",
    "        unique_values = np.unique(X[:, feature_index])\n",
    "        for threshold in unique_values:\n",
    "            gain = information_gain(X, y, feature_index, threshold)\n",
    "            if gain > best_gain:\n",
    "                best_gain = gain\n",
    "                best_feature_index = feature_index\n",
    "                best_threshold = threshold\n",
    "\n",
    "    return best_feature_index, best_threshold\n",
    "\n",
    "def decision_tree_train(X, y, depth=0, max_depth=5):\n",
    "    # Stopping criteria\n",
    "    if depth >= max_depth or len(np.unique(y)) == 1:\n",
    "        return np.bincount(y).argmax()\n",
    "\n",
    "    # Find best split\n",
    "    best_feature_index, best_threshold = find_best_split(X, y)\n",
    "    if best_feature_index is None:\n",
    "        return np.bincount(y).argmax()\n",
    "\n",
    "    # Split data\n",
    "    left_indices = np.where(X[:, best_feature_index] <= best_threshold)\n",
    "    right_indices = np.where(X[:, best_feature_index] > best_threshold)\n",
    "\n",
    "    left_subtree = decision_tree_train(X[left_indices], y[left_indices], depth + 1, max_depth)\n",
    "    right_subtree = decision_tree_train(X[right_indices], y[right_indices], depth + 1, max_depth)\n",
    "\n",
    "    return (best_feature_index, best_threshold), left_subtree, right_subtree\n",
    "\n",
    "# Train decision tree model\n",
    "tree_model = decision_tree_train(X, y)\n",
    "print(\"Decision Tree Model:\", tree_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b00342",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
